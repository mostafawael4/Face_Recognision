{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liberaries Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "from deepface import DeepFace\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 face(s) in this photograph.\n"
     ]
    }
   ],
   "source": [
    "# Load an image from file\n",
    "image = face_recognition.load_image_file(\".\\Images\\group\\group_image.png\")\n",
    "\n",
    "# Detect all faces in the image\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "# Print the number of faces detected\n",
    "print(f\"Found {len(face_locations)} face(s) in this photograph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image from RGB (used by face_recognition) to BGR (used by OpenCV)\n",
    "image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Draw a rectangle around each face\n",
    "for (top, right, bottom, left) in face_locations:\n",
    "    cv2.rectangle(image_bgr, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Faces Found', image_bgr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the face encodings for each face\n",
    "face_encodings = face_recognition.face_encodings(image, face_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Iterate over the images in the Individuals folder\n",
    "for filename in os.listdir(\".\\images\\individuals\"):\n",
    "    if filename.endswith(\".jpeg\"):\n",
    "        known_image = face_recognition.load_image_file(f\".\\Images\\Individuals\\{filename}\")\n",
    "        known_encoding = face_recognition.face_encodings(known_image)[0]\n",
    "        known_face_encodings.append(known_encoding)\n",
    "        known_face_names.append(filename.replace(\".jpeg\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image with an unknown face\n",
    "unknown_image = face_recognition.load_image_file(\".\\Images\\group\\group_image.png\")\n",
    "\n",
    "# Find all face locations and face encodings in the unknown image\n",
    "unknown_face_locations = face_recognition.face_locations(unknown_image)\n",
    "unknown_face_encodings = face_recognition.face_encodings(unknown_image, unknown_face_locations)\n",
    "\n",
    "# Loop over each face found in the unknown image\n",
    "for face_encoding, face_location in zip(unknown_face_encodings, unknown_face_locations):\n",
    "    \n",
    "    # See if the face is a match for the known faces\n",
    "    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "    \n",
    "    name = \"Unknown\"\n",
    "\n",
    "    # If a match was found, use the known person's name\n",
    "    if True in matches:\n",
    "        first_match_index = matches.index(True)\n",
    "        name = known_face_names[first_match_index]\n",
    "\n",
    "    # Draw a box around the face\n",
    "    top, right, bottom, left = face_location\n",
    "    cv2.rectangle(unknown_image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "    # Label the image with the name\n",
    "    cv2.putText(unknown_image, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Face Recognition', unknown_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Face Attributes detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:01<00:00,  3.12it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.28it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.23it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.66it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.80it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.88it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.83it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.91it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.90it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.87it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Abdallah_Mohamed': [{'age': 23,\n",
      "                       'dominant_emotion': 'happy',\n",
      "                       'dominant_gender': 'Man',\n",
      "                       'dominant_race': 'middle eastern',\n",
      "                       'emotion': {'angry': 6.378863684444887e-06,\n",
      "                                   'disgust': 5.045902219280377e-07,\n",
      "                                   'fear': 0.0025336362942147282,\n",
      "                                   'happy': 99.31600685811084,\n",
      "                                   'neutral': 0.6674011170060108,\n",
      "                                   'sad': 0.013002829702739699,\n",
      "                                   'surprise': 0.0010477757029248354},\n",
      "                       'face_confidence': 0.91,\n",
      "                       'gender': {'Man': 99.99967813491821,\n",
      "                                  'Woman': 0.00031815122838452226},\n",
      "                       'race': {'asian': 1.2118784710764885,\n",
      "                                'black': 0.9098192676901817,\n",
      "                                'indian': 9.848291426897049,\n",
      "                                'latino hispanic': 30.095240473747253,\n",
      "                                'middle eastern': 31.431668996810913,\n",
      "                                'white': 26.503100991249084},\n",
      "                       'region': {'h': 273,\n",
      "                                  'left_eye': (749, 249),\n",
      "                                  'right_eye': (657, 246),\n",
      "                                  'w': 273,\n",
      "                                  'x': 568,\n",
      "                                  'y': 137}}],\n",
      " 'Bernard_Arnault': [{'age': 47,\n",
      "                      'dominant_emotion': 'angry',\n",
      "                      'dominant_gender': 'Man',\n",
      "                      'dominant_race': 'white',\n",
      "                      'emotion': {'angry': 91.61304781514691,\n",
      "                                  'disgust': 0.3112636692085069,\n",
      "                                  'fear': 3.1390739613219476,\n",
      "                                  'happy': 0.006872258791536617,\n",
      "                                  'neutral': 0.4792580666695873,\n",
      "                                  'sad': 4.412448296692866,\n",
      "                                  'surprise': 0.03804005327128191},\n",
      "                      'face_confidence': 0.95,\n",
      "                      'gender': {'Man': 99.98006224632263,\n",
      "                                 'Woman': 0.019936800526920706},\n",
      "                      'race': {'asian': 0.21654064767062664,\n",
      "                               'black': 0.24555083364248276,\n",
      "                               'indian': 0.7619684096425772,\n",
      "                               'latino hispanic': 7.185497879981995,\n",
      "                               'middle eastern': 20.63450515270233,\n",
      "                               'white': 70.95593810081482},\n",
      "                      'region': {'h': 120,\n",
      "                                 'left_eye': None,\n",
      "                                 'right_eye': None,\n",
      "                                 'w': 120,\n",
      "                                 'x': 95,\n",
      "                                 'y': 67}}],\n",
      " 'Bill_Gates': [{'age': 58,\n",
      "                 'dominant_emotion': 'angry',\n",
      "                 'dominant_gender': 'Man',\n",
      "                 'dominant_race': 'white',\n",
      "                 'emotion': {'angry': 82.33043762082605,\n",
      "                             'disgust': 7.626768706672167e-05,\n",
      "                             'fear': 14.133676691809095,\n",
      "                             'happy': 3.532084401597452,\n",
      "                             'neutral': 0.0022997673100370766,\n",
      "                             'sad': 0.0011788470560406118,\n",
      "                             'surprise': 0.00023580270780518503},\n",
      "                 'face_confidence': 0,\n",
      "                 'gender': {'Man': 96.17046117782593,\n",
      "                            'Woman': 3.8295403122901917},\n",
      "                 'race': {'asian': 2.271214598470251e-05,\n",
      "                          'black': 3.8117982762305974e-08,\n",
      "                          'indian': 9.932349816474884e-06,\n",
      "                          'latino hispanic': 0.03258092135224299,\n",
      "                          'middle eastern': 0.09833627854198558,\n",
      "                          'white': 99.86905454028303},\n",
      "                 'region': {'h': 473,\n",
      "                            'left_eye': None,\n",
      "                            'right_eye': None,\n",
      "                            'w': 473,\n",
      "                            'x': 0,\n",
      "                            'y': 0}}],\n",
      " 'Carlos_Slim_Helu': [{'age': 52,\n",
      "                       'dominant_emotion': 'happy',\n",
      "                       'dominant_gender': 'Man',\n",
      "                       'dominant_race': 'white',\n",
      "                       'emotion': {'angry': 7.1316441641045e-08,\n",
      "                                   'disgust': 1.4413657716653453e-18,\n",
      "                                   'fear': 3.923831259911026e-08,\n",
      "                                   'happy': 99.99891519546509,\n",
      "                                   'neutral': 0.0010832146472239401,\n",
      "                                   'sad': 2.478979044609475e-10,\n",
      "                                   'surprise': 3.088334477041599e-07},\n",
      "                       'face_confidence': 0.92,\n",
      "                       'gender': {'Man': 99.96567964553833,\n",
      "                                  'Woman': 0.034315892844460905},\n",
      "                       'race': {'asian': 0.775473564863205,\n",
      "                                'black': 0.12670623837038875,\n",
      "                                'indian': 1.0099041275680065,\n",
      "                                'latino hispanic': 10.468573123216629,\n",
      "                                'middle eastern': 22.183144092559814,\n",
      "                                'white': 65.43620228767395},\n",
      "                       'region': {'h': 129,\n",
      "                                  'left_eye': None,\n",
      "                                  'right_eye': None,\n",
      "                                  'w': 129,\n",
      "                                  'x': 145,\n",
      "                                  'y': 52}}],\n",
      " 'Elon_Musk': [{'age': 34,\n",
      "                'dominant_emotion': 'happy',\n",
      "                'dominant_gender': 'Man',\n",
      "                'dominant_race': 'asian',\n",
      "                'emotion': {'angry': 1.1242302275634703e-08,\n",
      "                            'disgust': 1.3687286494016493e-18,\n",
      "                            'fear': 4.4995373586663224e-10,\n",
      "                            'happy': 97.21547994429112,\n",
      "                            'neutral': 2.7829148283106293,\n",
      "                            'sad': 2.6528356346688293e-07,\n",
      "                            'surprise': 0.001601457543159774},\n",
      "                'face_confidence': 0.92,\n",
      "                'gender': {'Man': 99.73146319389343,\n",
      "                           'Woman': 0.26853419840335846},\n",
      "                'race': {'asian': 33.30698013305664,\n",
      "                         'black': 3.4057430922985077,\n",
      "                         'indian': 5.427328869700432,\n",
      "                         'latino hispanic': 24.275025725364685,\n",
      "                         'middle eastern': 8.83888378739357,\n",
      "                         'white': 24.746033549308777},\n",
      "                'region': {'h': 668,\n",
      "                           'left_eye': (1379, 579),\n",
      "                           'right_eye': (1149, 612),\n",
      "                           'w': 668,\n",
      "                           'x': 932,\n",
      "                           'y': 333}}],\n",
      " 'Jack_Ma': [{'age': 34,\n",
      "              'dominant_emotion': 'fear',\n",
      "              'dominant_gender': 'Man',\n",
      "              'dominant_race': 'white',\n",
      "              'emotion': {'angry': 25.811997056007385,\n",
      "                          'disgust': 5.170084080227274e-28,\n",
      "                          'fear': 54.761505126953125,\n",
      "                          'happy': 1.5395928889411215e-10,\n",
      "                          'neutral': 19.426435232162476,\n",
      "                          'sad': 6.452893330788356e-05,\n",
      "                          'surprise': 8.328142088328813e-16},\n",
      "              'face_confidence': 0,\n",
      "              'gender': {'Man': 87.47439980506897, 'Woman': 12.52560168504715},\n",
      "              'race': {'asian': 1.3496225699782372,\n",
      "                       'black': 0.337715819478035,\n",
      "                       'indian': 0.7685142103582621,\n",
      "                       'latino hispanic': 12.235289067029953,\n",
      "                       'middle eastern': 9.776079654693604,\n",
      "                       'white': 75.53277611732483},\n",
      "              'region': {'h': 233,\n",
      "                         'left_eye': None,\n",
      "                         'right_eye': None,\n",
      "                         'w': 391,\n",
      "                         'x': 0,\n",
      "                         'y': 0}}],\n",
      " 'Jeff_Bezos': [{'age': 37,\n",
      "                 'dominant_emotion': 'sad',\n",
      "                 'dominant_gender': 'Man',\n",
      "                 'dominant_race': 'white',\n",
      "                 'emotion': {'angry': 28.301414847373962,\n",
      "                             'disgust': 0.20275006536394358,\n",
      "                             'fear': 5.60418963432312,\n",
      "                             'happy': 0.09996638400480151,\n",
      "                             'neutral': 30.220091342926025,\n",
      "                             'sad': 34.835854172706604,\n",
      "                             'surprise': 0.7357343100011349},\n",
      "                 'face_confidence': 0.94,\n",
      "                 'gender': {'Man': 99.9863862991333,\n",
      "                            'Woman': 0.013613494229502976},\n",
      "                 'race': {'asian': 5.187125504016876,\n",
      "                          'black': 2.921084500849247,\n",
      "                          'indian': 5.6783705949783325,\n",
      "                          'latino hispanic': 20.033560693264008,\n",
      "                          'middle eastern': 27.553242444992065,\n",
      "                          'white': 38.626620173454285},\n",
      "                 'region': {'h': 91,\n",
      "                            'left_eye': None,\n",
      "                            'right_eye': None,\n",
      "                            'w': 91,\n",
      "                            'x': 144,\n",
      "                            'y': 24}}],\n",
      " 'Larry_Ellison': [{'age': 41,\n",
      "                    'dominant_emotion': 'angry',\n",
      "                    'dominant_gender': 'Man',\n",
      "                    'dominant_race': 'asian',\n",
      "                    'emotion': {'angry': 37.24592924118042,\n",
      "                                'disgust': 5.683257058262825,\n",
      "                                'fear': 17.678695917129517,\n",
      "                                'happy': 0.2646475564688444,\n",
      "                                'neutral': 14.047569036483765,\n",
      "                                'sad': 24.50806051492691,\n",
      "                                'surprise': 0.5718388594686985},\n",
      "                    'face_confidence': 0.93,\n",
      "                    'gender': {'Man': 99.04099702835083,\n",
      "                               'Woman': 0.9589989669620991},\n",
      "                    'race': {'asian': 40.73759317398071,\n",
      "                             'black': 10.534483939409256,\n",
      "                             'indian': 15.150125324726105,\n",
      "                             'latino hispanic': 20.574641227722168,\n",
      "                             'middle eastern': 5.419602990150452,\n",
      "                             'white': 7.583555579185486},\n",
      "                    'region': {'h': 64,\n",
      "                               'left_eye': None,\n",
      "                               'right_eye': None,\n",
      "                               'w': 64,\n",
      "                               'x': 163,\n",
      "                               'y': 31}}],\n",
      " 'Larry_Page': [{'age': 24,\n",
      "                 'dominant_emotion': 'neutral',\n",
      "                 'dominant_gender': 'Man',\n",
      "                 'dominant_race': 'white',\n",
      "                 'emotion': {'angry': 3.8204073905944824,\n",
      "                             'disgust': 0.0012996045370528009,\n",
      "                             'fear': 0.05951942293904722,\n",
      "                             'happy': 1.7419662326574326,\n",
      "                             'neutral': 90.8681333065033,\n",
      "                             'sad': 3.1853381544351578,\n",
      "                             'surprise': 0.3233378520235419},\n",
      "                 'face_confidence': 0.92,\n",
      "                 'gender': {'Man': 99.91961121559143,\n",
      "                            'Woman': 0.0803928473033011},\n",
      "                 'race': {'asian': 1.3089824770113854,\n",
      "                          'black': 0.4460215968165878,\n",
      "                          'indian': 3.3429961391691996,\n",
      "                          'latino hispanic': 18.484360756354242,\n",
      "                          'middle eastern': 25.147539359624464,\n",
      "                          'white': 51.2700913822542},\n",
      "                 'region': {'h': 96,\n",
      "                            'left_eye': (202, 89),\n",
      "                            'right_eye': (168, 91),\n",
      "                            'w': 96,\n",
      "                            'x': 139,\n",
      "                            'y': 52}}],\n",
      " 'Mark_Zukerberg': [{'age': 34,\n",
      "                     'dominant_emotion': 'happy',\n",
      "                     'dominant_gender': 'Woman',\n",
      "                     'dominant_race': 'white',\n",
      "                     'emotion': {'angry': 1.402378494770205e-12,\n",
      "                                 'disgust': 4.4871832514165026e-27,\n",
      "                                 'fear': 9.155249908987936e-16,\n",
      "                                 'happy': 100.0,\n",
      "                                 'neutral': 3.189859043573051e-06,\n",
      "                                 'sad': 1.443973114906641e-11,\n",
      "                                 'surprise': 1.3284874589292173e-08},\n",
      "                     'face_confidence': 0.94,\n",
      "                     'gender': {'Man': 26.580575108528137,\n",
      "                                'Woman': 73.41942191123962},\n",
      "                     'race': {'asian': 0.24163080379366875,\n",
      "                              'black': 0.025132347946055233,\n",
      "                              'indian': 0.4272217396646738,\n",
      "                              'latino hispanic': 15.532532334327698,\n",
      "                              'middle eastern': 18.303953111171722,\n",
      "                              'white': 65.46952724456787},\n",
      "                     'region': {'h': 113,\n",
      "                                'left_eye': (229, 84),\n",
      "                                'right_eye': (194, 80),\n",
      "                                'w': 113,\n",
      "                                'x': 154,\n",
      "                                'y': 38}}],\n",
      " 'Warren_Buffett': [{'age': 44,\n",
      "                     'dominant_emotion': 'happy',\n",
      "                     'dominant_gender': 'Man',\n",
      "                     'dominant_race': 'white',\n",
      "                     'emotion': {'angry': 0.0006734751423209673,\n",
      "                                 'disgust': 6.875067853749249e-13,\n",
      "                                 'fear': 0.027919866261072457,\n",
      "                                 'happy': 95.83462476730347,\n",
      "                                 'neutral': 4.135195165872574,\n",
      "                                 'sad': 0.0011482700756459963,\n",
      "                                 'surprise': 0.00043960112634522375},\n",
      "                     'face_confidence': 0.93,\n",
      "                     'gender': {'Man': 99.86355900764465,\n",
      "                                'Woman': 0.13643514830619097},\n",
      "                     'race': {'asian': 0.5063895929438361,\n",
      "                              'black': 0.049324601363810734,\n",
      "                              'indian': 0.23208439377434328,\n",
      "                              'latino hispanic': 7.3870761827139635,\n",
      "                              'middle eastern': 5.6063760839252375,\n",
      "                              'white': 86.21874615922616},\n",
      "                     'region': {'h': 163,\n",
      "                                'left_eye': None,\n",
      "                                'right_eye': None,\n",
      "                                'w': 163,\n",
      "                                'x': 144,\n",
      "                                'y': 13}}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_result = {}\n",
    "\n",
    "for filename in os.listdir(\".\\images\\individuals\"):\n",
    "    if filename.endswith(\".jpeg\"):\n",
    "        known_image = face_recognition.load_image_file(f\".\\Images\\Individuals\\{filename}\")\n",
    "        known_image = cv2.cvtColor(known_image, cv2.COLOR_BGR2RGB)\n",
    "        try:\n",
    "            detector = DeepFace.analyze(known_image, enforce_detection=False)\n",
    "            final_result[filename.replace(\".jpeg\", \"\")] = detector\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "pprint.pprint(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop over each frame from the video until interrupted (e.g., by pressing 'q')\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame from BGR (OpenCV) to RGB (face_recognition)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Find all face locations and encodings in the frame\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # Loop over each face in the frame\n",
    "    for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # Check if there is a match and get the corresponding name\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            name = known_face_names[first_match_index]\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        top, right, bottom, left = face_location\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "        # Label the face with the name\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the video feed\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Facial Attribute Detection (Emotion Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize FER emotion detector\n",
    "emotion_detector = FER()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Check if frame is successfully captured\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Detect emotions in the frame\n",
    "    emotions = emotion_detector.detect_emotions(frame)\n",
    "\n",
    "    # Loop through detected faces and emotions\n",
    "    for emotion_data in emotions:\n",
    "        # Get bounding box (face location) and emotions for each face\n",
    "        (top, right, bottom, left) = emotion_data['box']\n",
    "        emotion, score = max(emotion_data['emotions'].items(), key=lambda item: item[1])\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "        # Display the emotion and score near the face\n",
    "        text = f\"{emotion}: {score:.2f}\"\n",
    "        cv2.putText(frame, text, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Facial Landmarks Detection (DeepFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Initialize Dlib's face detector and facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  \n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Detect faces using Dlib\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Analyze attributes using DeepFace for each detected face\n",
    "    for face in faces:\n",
    "        # Get bounding box coordinates for the face\n",
    "        x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
    "\n",
    "        # Extract the face region from the frame and convert it to an image\n",
    "        face_image = frame[y:y+h, x:x+w]\n",
    "        face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Analyze the face with DeepFace to detect age, gender, emotion\n",
    "        try:\n",
    "            analysis = DeepFace.analyze(face_image, enforce_detection=False)\n",
    "            age = analysis[0]['age']\n",
    "            gender = analysis[0]['dominant_gender']\n",
    "            emotion = analysis[0]['dominant_emotion']\n",
    "            # race = analysis[0]['dominant_race']\n",
    "            \n",
    "            # Display the detected age, gender, and emotion\n",
    "            cv2.putText(frame, f'Age: {age}', (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Gender: {gender}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error analyzing face: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # Detect and display facial landmarks using Dlib\n",
    "        landmarks = predictor(gray, face)\n",
    "        for n in range(0, 68):  # Loop over the 68 facial landmarks\n",
    "            x_point = landmarks.part(n).x\n",
    "            y_point = landmarks.part(n).y\n",
    "            cv2.circle(frame, (x_point, y_point), 2, (0, 255, 0), -1)\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the video feed\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Beast MODE\n",
    "#### (Real-Time Face Recognition with facial landmarks,age,gender and emotions Predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.73it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:01<00:00,  3.96it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:01<00:00,  3.81it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.15it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.07it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:01<00:00,  3.97it/s]  \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from deepface import DeepFace\n",
    "import face_recognition\n",
    "\n",
    "# Initialize Dlib's face detector and facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale for Dlib\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Convert the frame from BGR to RGB for face_recognition\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces using Dlib\n",
    "    dlib_faces = detector(gray)\n",
    "\n",
    "    # Detect faces using face_recognition\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # Process each detected face\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # Face recognition\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        name = \"Unknown\"\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            name = known_face_names[first_match_index]\n",
    "\n",
    "        # Extract the face region\n",
    "        face_image = frame[top:bottom, left:right]\n",
    "        face_image_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Analyze the face with DeepFace\n",
    "        try:\n",
    "            analysis = DeepFace.analyze(face_image_rgb, enforce_detection=False)\n",
    "            age = analysis[0]['age']\n",
    "            gender = analysis[0]['dominant_gender']\n",
    "            emotion = analysis[0]['dominant_emotion']\n",
    "\n",
    "            # Display results\n",
    "            cv2.putText(frame, f'Name: {name}', (left, top - 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Age: {age}', (left, top - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Gender: {gender}', (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (left, bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error analyzing face: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "        # Detect and display facial landmarks using Dlib\n",
    "        dlib_face = dlib.rectangle(left, top, right, bottom)\n",
    "        landmarks = predictor(gray, dlib_face)\n",
    "        for n in range(0, 68):  # Loop over the 68 facial landmarks\n",
    "            x_point = landmarks.part(n).x\n",
    "            y_point = landmarks.part(n).y\n",
    "            cv2.circle(frame, (x_point, y_point), 2, (0, 255, 0), -1)\n",
    "\n",
    "    # Display the video feed\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE can make this a functional Function in a ELearning platform to track student learning.\n",
    "- based on the studednt profile bicture. and name\n",
    "- generate a report about the student learning session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work & Potential Integrations\n",
    "\n",
    "1. **Attention Mechanism**:\n",
    "   - You could implement an attention mechanism in your face recognition system that allows the model to focus on specific parts of the input image or sequence of frames. This could enhance the accuracy of facial attribute detection and recognition.\n",
    "   - Attention can help in interpreting which areas of the face are most relevant when making predictions, possibly improving the results of your facial analysis.\n",
    "\n",
    "2. **Image Captioning**:\n",
    "   - Use NLP to generate descriptive captions for images detected in the video feed. For instance, once a face is recognized and analyzed, you could create a textual summary of the analysis results (e.g., \"This is John, a 25-year-old male, looking happy.\").\n",
    "   - You could combine visual features extracted from the image with textual descriptions for enhanced data representation.\n",
    "\n",
    "3. **Sentiment Analysis**:\n",
    "   - Analyze the emotions detected using DeepFace and combine this with NLP techniques to evaluate sentiment in any text associated with the detected individual. For example, if a recognized person has spoken text (like a transcript), you could analyze how their spoken words relate to their facial expressions.\n",
    "   - This could be particularly useful in applications like customer service or interactive AI where understanding both visual and textual cues is important.\n",
    "\n",
    "4. **Using Grad-CAM for Interpretability**:\n",
    "   - Grad-CAM (Gradient-weighted Class Activation Mapping) can be utilized to visualize which parts of the image influenced the model’s predictions. After facial analysis, you can overlay Grad-CAM heatmaps on the detected faces to show which facial features led to specific attributes or recognition results.\n",
    "   - This can improve the interpretability of your model, allowing users to understand why certain predictions were made.\n",
    "\n",
    "5. **Multimodal Data Analysis**:\n",
    "   - Combine text data (e.g., transcripts, comments) with visual data (video or images) for a more holistic analysis. For example, when analyzing a video, you can detect faces and simultaneously transcribe speech, analyzing how facial expressions align with spoken content.\n",
    "\n",
    "### Example Workflow\n",
    "\n",
    "1. **Face Detection and Analysis**:\n",
    "   - Use the existing face recognition system to detect and analyze faces in real-time.\n",
    "\n",
    "2. **Text Data Integration**:\n",
    "   - As faces are recognized, pull any associated textual data (like comments or transcripts) and analyze it using NLP techniques.\n",
    "\n",
    "3. **Generate Captions**:\n",
    "   - Use NLP to generate captions or textual descriptions based on the results of facial analysis (age, gender, emotions).\n",
    "\n",
    "4. **Visualize Attention with Grad-CAM**:\n",
    "   - Apply Grad-CAM to visualize which facial regions contributed to the attribute predictions, displaying these overlays on the video feed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
